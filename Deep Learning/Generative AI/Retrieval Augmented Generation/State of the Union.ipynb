{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc02f46-74a0-4efc-b61d-e1d4fada18e4",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9610f-aa6a-4914-b0d5-115fbbdd53b7",
   "metadata": {},
   "source": [
    "For more intuition about what Retrieval Augmented Generation (RAG) is and why RAG systems are effective for many business use cases, I'd recommend checking out this blog post by Nicole Choi:\n",
    "\n",
    "https://github.blog/ai-and-ml/generative-ai/what-is-retrieval-augmented-generation-and-what-does-it-do-for-generative-ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c90427-90c1-47da-8344-4edc2d256842",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c7c257-8fcb-41cd-a8f5-37ec86e5a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f428cf3-65cb-43de-93d6-27eff2bb0971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Check if GPU is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057f1b8-4880-498d-a61a-31d6953c239d",
   "metadata": {},
   "source": [
    "## Download and Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63417c-a3ad-4a75-a085-536b52be2565",
   "metadata": {},
   "source": [
    "In this isntance, we will be using Meta's Llama-2 model. you can change this to whatever model you want to use. You can request access to the models directly from Meta's website, or you can request access and download them from Kaggle.\n",
    "\n",
    "Meta: https://llama.meta.com/llama2/\n",
    "\n",
    "Kaggle: https://www.kaggle.com/models/metaresearch/llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d8a4f4-1cd2-4591-b53d-68a98810195b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### This model is approximately 25GB downloaded from Kaggle\n",
    "#### Uncomment to run\n",
    "#path = kagglehub.model_download(\"metaresearch/llama-2/pyTorch/7b-chat-hf\")\n",
    "#print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26866562-fb11-40e0-a5fe-24b868404669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Setup quantization configuration to load the large model with less GPU memory\n",
    "model_id = 'C:/Users/oshan/.cache/kagglehub/models/metaresearch/llama-2/pyTorch/7b-chat-hf/1'\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa1404f-a2b8-4d45-9132-5c60ad127d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:50<00:00, 25.12s/it]\n",
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 55.876 sec.\n"
     ]
    }
   ],
   "source": [
    "#### Load model\n",
    "time_1 = time()\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_2 = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c26de-1ea1-4d48-bdbb-edadd9bbb970",
   "metadata": {},
   "source": [
    "## Prepare Pipeline and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0444109b-a6b4-4de0-9a00-599f5df61cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 4.242 sec.\n"
     ]
    }
   ],
   "source": [
    "#### Prepare query pipeline\n",
    "time_1 = time()\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",)\n",
    "time_2 = time()\n",
    "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b6900a-386f-4fbf-a5b2-3f570e060f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        prompt_to_test: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
    "    time_1 = time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_2 = time()\n",
    "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8e5475-bbd7-4cb4-8115-49c1140dbdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\transformers\\generation\\utils.py:1364: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 7.163 sec.\n",
      "Result: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words. Thank you.\n",
      "The State of the Union address is a yearly speech given by the President of the United States to Congress, outlining the current state of the country, its challenges, and its goals for the future. In the speech, the President typically highlights the administration's achievements, makes policy proposals, and calls for bipartisan cooperation and unity.\n"
     ]
    }
   ],
   "source": [
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aef3b8-7063-4e57-baf8-06da29d52f08",
   "metadata": {},
   "source": [
    "## Loading Pipeline into HuggingFace API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc91473-7f11-40cf-9e0f-691f38674ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\\n\\nIn President Biden's 2023 State of the Union address, he focused on several key issues, including:\\n\\n1. Economic growth and job creation, highlighting the need for investments in infrastructure, education, and research.\\n2. Healthcare, with a proposal to lower prescription drug prices and improve access to affordable healthcare.\\n3. Climate change, with a call to action to reduce carbon emissions and invest in clean energy.\\n4. Immigration, with a proposal to provide a pathway to citizenship for undocumented immigrants.\\n5. Gun violence, with a call for stricter gun control measures and increased funding for mental health treatment.\\n6. Social justice, with a focus on addressing systemic racism and inequality in the criminal justice system.\\n\\nOverall, President Biden's address emphasized the need for bipartisan cooperation and action to address the nation's challenges and improve the lives of all Americans.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "# checking again that everything is working fine\n",
    "llm(prompt=\"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c7e62-b524-44c7-9f04-792be3da32b4",
   "metadata": {},
   "source": [
    "## Loading Data for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac91ba9-5dba-40d8-aedd-70413d2b7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading our text data for data preprocessing\n",
    "loader = TextLoader(\"biden-sotu-2023-planned-official.txt\",\n",
    "                    encoding=\"utf8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a90cbaa5-0aee-4271-a84d-f28086c7feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Splitting data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5a9a7-4e42-4822-a555-a202dde9b294",
   "metadata": {},
   "source": [
    "More information regarding sentence transformers can be found on HuggingFace: https://huggingface.co/sentence-transformers/all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891dc121-4cd2-4de0-a1b3-93ca17ae52b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#### Loading sentence embeddings from HuggingFace\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058c34ac-c6d3-4455-9532-5ba31c44b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Storing data in vector database named \"chroma_db\"\n",
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"SOTU_chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4ba32-fb3c-48f0-93c0-674e5472041c",
   "metadata": {},
   "source": [
    "## Creating Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "980fef02-ae96-48c3-8327-3db5ed309e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f6be031-a29d-4c22-8059-3956d9e43252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing Retriever\n",
    "def test_rag(qa, query):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    time_1 = time()\n",
    "    result = qa.run(query)\n",
    "    time_2 = time()\n",
    "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
    "    print(\"\\nResult: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2210c015-76c7-4172-b848-42b4e70fe803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oshan\\anaconda3\\envs\\RAG\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Inference time: 10.546 sec.\n",
      "\n",
      "Result:  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "peace,not just in Europe, but everywhere. Before I came to office, the story was about how the People’s Republic of China was increasing its power and America was falling in the world. Not anymore. I’ve made clear with President Xi that we seek competition, not conflict. I will make no apologies that we are investing to make America strong. Investing in American innovation, in industries that will define the future, and that China’s government is intent on dominating. Investing in our alliances and working with our allies to protect our advanced technologies so they’re not used against us. Modernizing our military to safeguard stability and deter aggression. Today, we’re in the strongest position in decades to compete with China or anyone else in the world. I am committed to work with China where it can advance American interests and benefit the world. But make no mistake: as we made clear last week, if China’s threatens our sovereignty, we will act to protect our country. And we did.\n",
      "\n",
      "Americans, we meet tonight at an inflection point. One of those moments that only a few generations ever face, where the decisions we make now will decide the course of this nation and of the world for decades to come. We are not bystanders to history. We are not powerless before the forces that confront us. It is within our power, of We the People. We are facing the test of our time and the time for choosing is at hand. We must be the nation we have always been at our best. Optimistic. Hopeful. Forward-looking. A nation that embraces, light over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the\n",
      "\n",
      "forward. Of never giving up. A story that is unique among all nations. We are the only country that has emerged from every crisis stronger than when we entered it. That is what we are doing again. Two years ago, our economy was reeling. As I stand here tonight, we have created a record 12 million new jobs, more jobs created in two years than any president has ever created in four years. Two years ago, COVID had shut down our businesses, closed our schools, and robbed us of so much. Today, COVID no longer controls our lives. And two years ago, our democracy faced its greatest threat since the Civil War. Today, though bruised, our democracy remains unbowed and unbroken. As we gather here tonight, we are writing the next chapter in the great American story, a story of progress and resilience. When world leaders ask me to define America, I define our country in one word: Possibilities. You know, we’re often told that Democrats and Republicans can’t work together. But over these past two\n",
      "\n",
      "Question: What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\n",
      "Helpful Answer: The State of the Union in 2023 focused on several key topics, including the nation's economic strength, the competition with China, and the need to come together as a nation to face the challenges ahead. The President emphasized the importance of American innovation, industries, and military modernization to ensure the country's safety and stability. The President also highlighted the nation's resilience and optimism, urging Americans to see each other as fellow citizens and to work together to overcome the challenges facing the country.\n"
     ]
    }
   ],
   "source": [
    "query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c95ff6-6e25-4012-ab6e-a1fdefb7ea5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8b21a-a8a1-4b5c-a8cc-fea5ab84882f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "310082f3-ef07-4f08-955b-5cb6c2ee6d6c",
   "metadata": {},
   "source": [
    "## A More Personal RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1523ba6-e4e6-4745-ae9c-e7b9f2f9716f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please explain who Noah Oshana is. Keep it in 100 words.\\nNoah Oshana is a 21-year-old American social media personality and content creator. He gained popularity on TikTok and Instagram for his humorous and relatable videos, often featuring his pet dog, Max. Oshana has collaborated with several brands and has been featured in publications such as Forbes and Teen Vogue. He has over 3 million followers on TikTok and over 1 million followers on Instagram.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "# checking again that everything is working fine\n",
    "llm(prompt=\"Please explain who Noah Oshana is. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beab09d3-621a-42e4-9a1c-8a860da5ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lol wtf? this is obviously not me...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06e60866-17e6-49e8-9ec2-fdfdc0bdebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"Noah.txt\",\n",
    "                    encoding=\"utf8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94d76e6e-1395-45b7-ad7f-c2b7ef000747",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45aa08bd-b6ac-49ce-8f0e-fa182b5c5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59664b6a-4af5-411f-93ab-c30985879695",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"noah_chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd560a5f-1d83-454c-bfb3-30347e98223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fde1628-aef9-4b5a-9493-43192881105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag(qa, query):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    time_1 = time()\n",
    "    result = qa.run(query)\n",
    "    time_2 = time()\n",
    "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
    "    print(\"\\nResult: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae4e35eb-655a-4caf-8ec6-16f8a152a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Please explain who Noah Oshana is. Keep it in 100 words.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Inference time: 8.735 sec.\n",
      "\n",
      "Result:  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Question: What are Noah Oshana's hobbies?\n",
      "Answer: Noah enjoys all things sports related. He personally enjoys playing golf in his free time. He is a very active individual who spends 6/7 days of the week lifting weights. He is also a luxury watch and bourbon connoisseur.\n",
      "\n",
      "Question: Does Noah Oshana have any family? \n",
      "Answer: Noah is the son of Robert and Susan Oshana. He has one older brother, Samuel Oshana.\n",
      "\n",
      "Question: What did Noah Oshana study at the University of Colorado at Boulder?\n",
      "Answer: Noah recieved his Masters of Science in Data Science at the University of Colorado at Boulder.\n",
      "\n",
      "Question: What is Noah Oshana's work experience?\n",
      "Answer: Noah has worked for NXP Semiconductors as a machine learning software engineer.\n",
      "\n",
      "Question: Where can I find Noah Oshana's LinkedIn profile?  \n",
      "Answer: Noah's LinkedIn can be found here: https://www.linkedin.com/in/noahoshana/\n",
      "\n",
      "Question: Where can I find Noah Oshana's machine learning coding projects?\n",
      "Answer: Noah has a machine learning GitHub repository that contains many of the \"playground\" or Proof of Concept projects he has worked on. This repository can be found here: https://github.com/NoahOshana17\n",
      "\n",
      "Question: When was Noah Oshana born?\n",
      "Answer: Noah Oshana was born on June 17th 1998\n",
      "\n",
      "Question: Where is Noah Oshana from?\n",
      "Answer: Noah Oshana was born in Dallas, Texas. Before he turned a year old, his family moved to Sugar Land, Texas. After completing his 4th grade year, his family moved to Austin, TX where he spend the remainder of his childhood and currently lives today. \n",
      "\n",
      "Question: What High School did Noah Oshana attend? \n",
      "Answer: Noah Oshana attended Lake Travis High School.\n",
      "\n",
      "Question: Where did Noah Oshana go to college?\n",
      "Answer: Noah Oshana went to Baylor University for his undergraduate education where he studied Economics. He later attended the University of Colorado at Boulder and recieved his Masters in Science in Data Science where he focused in machine learning. \n",
      "\n",
      "Question: What did Noah Oshana study at Baylor University?\n",
      "Answer: Noah recieved his Bachelors of Arts in Economics at Baylor University.\n",
      "\n",
      "Question: Please explain who Noah Oshana is. Keep it in 100 words.\n",
      "Helpful Answer: Noah Oshana is a 23 year old data scientist and machine learning engineer from Austin, Texas. He is the son of Robert and Susan Oshana, and has one older brother, Samuel. Noah has a Master's degree in Data Science from the University of Colorado at Boulder, and has worked for NXP Semiconductors as a machine learning software engineer. He is also an avid golfer and luxury watch collector.\n"
     ]
    }
   ],
   "source": [
    "query = \"Please explain who Noah Oshana is. Keep it in 100 words.\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cac7e-9169-4a12-ad16-2001f5194f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
